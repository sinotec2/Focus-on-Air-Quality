# Tuesday, August 29, 2023 NLP

## Definition

### amazon

- [什麼是自然語言處理 (NLP)？](https://aws.amazon.com/tw/what-is/nlp/)
  > 自然語言處理 (NLP) 是一種機器學習技術，讓電腦能夠解譯、操縱及理解人類語言。如今，組織擁有來自各種通訊管道的大量語音和文字資料，例如電子郵件、簡訊、社交媒體新聞摘要、影片、音訊等。他們使用 NLP 軟體來自動處理此資料，分析訊息中的意圖或情緒，並即時回應人類通訊。
  - 為什麼 NLP 很重要？
  - 什麼是企業的 NLP 使用案例？
  - NLP 如何運作？
  - 什麼是 NLP 任務？
  - 自然語言處理採用哪些方法？
  > 針對想要在其業務中建立標準 NLP 解決方案的客戶，Amazon SageMaker 可透過全受管基礎設施、工具和工作流程，輕鬆準備資料，以及建置、訓練和部署機器學習模型，包括適用於商業分析師的無程式碼服務。藉助 Amazon SageMaker 上的 *Hugging Face*，您可以部署和微調來自 *Hugging Face* 的預先訓練模型，*Hugging Face* 是稱為 Transformers 的自然語言處理 (NLP) 模型的開放原始碼供應商，將設定和使用這些 NLP 模型所需的時間從數週縮短至幾分鐘。

立即建立 AWS 帳戶，開始使用自然語言處理 (NLP)。

## Hugging Face

- The AI community building the future.
- The platform where the machine learning community collaborates on models, datasets, and applications.
- If you need to create a repo from the command line (skip if you created a repo from the website)
- HuggingGPT爆紅，Hugging Face又是什麼？它正在拆掉OpenAI的圍牆，要當AI界的Github、[36Kr 發表於 2023年4月30日 16:00 電腦王->](https://www.techbang.com/posts/105484-hugginggpt-is-on-fire-what-is-hugging-face-hugging-face-a-2)
  >  
- Hugging Face 架構與三大神器 by [大魔術熊貓工程師2022-09-17 20:35:25@ithelp.ithome](https://ithelp.ithome.com.tw/articles/10291757)

### Getting started

- with our git and git-lfs interface

```bash
pip install huggingface_hub
#You already have it if you installed transformers or datasets

huggingface-cli login

# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}
```

### Clone your model or dataset locally

```bash
#Make sure you have git-lfs installed
#(https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name
```

- Then add, commit and push any file you want, including larges files

```bash
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push
```

- In most cases, if you're using one of the compatible libraries, your repo will then be accessible from code, through its identifier: username/repo_name

For example for a transformers model, anyone can load it with:

```python
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```

### 中研院詞語庫

- [CKIP Lab](https://huggingface.co/ckiplab)
  1. 繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）[bert-base-chinese-ner](https://huggingface.co/ckiplab/bert-base-chinese-ner)
  2. (same as above, fill-mask and lm-head)[bert-base-chinese](https://huggingface.co/ckiplab/bert-base-chinese)
  3. [albert-tiny-chinese-ws](https://huggingface.co/ckiplab/albert-tiny-chinese-ws)

### 法律相關

- 宝锣法律大模型1.0 [xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b](https://huggingface.co/xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b)
  - 綁定使用GPU
- LawBERT
  - Dataset Card for "LawBERT_data" [LawBERT-tw/LawBERT_data ](https://huggingface.co/datasets/LawBERT-tw/LawBERT_data)
  - model: [LawBERT-s](https://huggingface.co/LawBERT-tw/LawBERT-s)
    - BertForMaskedLM
- JessyTsu1
  - Model:[ChatLaw-13B](https://huggingface.co/JessyTsu1/ChatLaw-13B)

### 工程相關

Data comes from Published Texas A&M Engineering News and was used to train a MLM @3epochs 500([houck2040/engineering](https://huggingface.co/datasets/houck2040/engineering))

### 環境相關

- Korean Dataset Construction for Relation Extraction Task on Natures and Environments 韓語詞彙關係 ([kimcando/KOR-RE-natures-and-environments](https://huggingface.co/datasets/kimcando/KOR-RE-natures-and-environments))
- [climatebert/environmental_claims](https://huggingface.co/datasets/climatebert/environmental_claims)
  - label: the label (0 -> no environmental claim, 1 -> environmental claim)

### jondurbin

- [jondurbin/airoboros-13b-gpt4-1.4](https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4)有30多萬次下載，是gpt4.1.4最新的延伸版本

### FastChat

這個界面獲得2萬多個github好評。會需要3個埠(不能重複。程式內設都會是21001，只能用一次)
1. 控制埠($addc=http://${ip}:21001內設)，搭配controller-address(作業埠需要)/controller-url(UI埠需要)之引數
2. 作業埠($add=http://${ip}:$port)。引數worker-address只需設定一次。
3. UI埠($addu=http://${ip}:$port1)，為使用者最後看到的界面。

安裝與其餘設定詳見[jondurbin/FastChat](https://github.com/jondurbin/FastChat)。此處詳解devp(ip=200.200.32.195)上的啟動方式。([結果位址](http://200.200.32.195:55081))


- 控制埠
  - 如不指定host，則會是內設值localhost。

```bash
python3 -m fastchat.serve.controller --host $ip &
```

- 模型作業埠
  - model-path可以指定到特定的模型。不一定是lmsys。
  - 上接控制埠、下啟UI埠。
  - 同一個控制埠及UI埠可以(同時)使用多個模型，只要開在不同的位址就好了。
  - 必須指定device，否則內設為gpu


```bash
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5-16k \
--host $ip \
--worker-address $add \
--controller-address $addc --port $port --device cpu &
```

- 測試模型是否正常運作
  - 需指定工作埠位址。啟動模型也必須與工作模型一致。
  - 此處不需再指定device

```bash
python3 -m fastchat.serve.test_message --model-name vicuna-7b-v1.5-16k --worker-address $add
```

- webui程式埠
  - 需要設定2組位址，一個是控制埠(controller-url)，一個是使用者會連上的最後位址。

```bash
python3 -m fastchat.serve.gradio_web_server --host $ip --port $port1 --controller-url $addc
```

## Pile

- The Pile: An 800GB Dataset of Diverse Text for Language Modeling @[eleuther(2020)](https://pile.eleuther.ai/)
- `https://the-eye.eu/public/AI/pile_preliminary_components`目錄已經不存在了，改成`https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/`

### txt jsonize

- 本文中有哪些主體、各主體間的關聯性為何?寫成圖譜節點和邊緣、並以JSON 架構方式及中文來表達。
- 本文中問題說明提到了哪些事件、各事件的因果關係為何?逐一以中文條列。
- 本文中提到的因應對策有哪些?
- 前述事件3的因果關係為何?是否應用在因應對策中?
- 逐一檢討前述10項因果關係，是否應用在因應對策中?

## training parameters

### training arguments

```python
batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
```

- day23

```python
args = Seq2SeqTrainingArguments( 
    output_dir=f"{model_name}-finetuned", 
    num_train_epochs=1, 
    warmup_steps=100,
    per_device_train_batch_size=10, 
    per_device_eval_batch_size=10,
    weight_decay=0.01, 
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=100, 
    save_steps=1e6,
    gradient_accumulation_steps=64,
    report_to="azure_ml"
)
```

### compute_metrics

- day16

```python
from sklearn.metrics import accuracy_score, f1_score
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}
```

- day23

```python
from datasets import load_metric
rouge_metric = load_metric("rouge")
def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # 這裡把 DataCollatorForSeq2Seq 會填入的 -100 排除掉
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]

    result = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

### trainer settings

- day16

```python
```

- day23

```python
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset= dataset_pt["train"],
    eval_dataset = dataset_pt["valid"],
    data_collator=seq2seq_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```

## pipeline tasks


available tasks are 
- 'audio-classification', 
- 'automatic-speech-recognition', 
- 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 
- 'fill-mask', 
- 'image-classification', 'image-segmentation', 'image-to-text', 
- 'mask-generation', 'ner', 'object-detection', 
- 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 
- 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 
- 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 
- 'zero-shot-audio-classification', 'zero-shot-classification', 
- 'zero-shot-image-classification', 'zero-shot-object-detection', 
- 'translation_XX_to_YY'

## ChatPDF

- 官網：[ttps://www.chatpdf.com](ttps://www.chatpdf.com)

Free|Plus
-|-
$0/mo|$5/mo
120 pages/PDF|2,000 pages/PDF
10 MB/PDF|32 MB/PDF
3 PDFs/day|50 PDFs/day
50 questions/day|1000 questions/day

### chatbot 對話紀錄 資料庫

建立一個用於保存 chatbot 對話紀錄的資料庫是一個有用的做法，這樣你可以隨時檢視和分析過去的對話，也可以改進你的 chatbot。以下是一個簡單的範例，展示如何設計一個基本的對話紀錄資料庫：

- 資料表（Table）設計：創建一個名為 "conversations" 的資料表，用於保存整個對話的紀錄。資料表的欄位可以包括：
  - conversation_id (對話的唯一識別符)
  - user_id (使用者的唯一識別符)
  - bot_id (chatbot 的唯一識別符)
  - timestamp (對話的時間戳)
  - user_message (使用者的訊息)
  - bot_message (chatbot 的回覆)
- 資料庫設定：使用適合你的應用的關聯式資料庫，例如 MySQL、PostgreSQL、SQLite 等。你也可以考慮使用 NoSQL 資料庫，如 MongoDB，如果你需要更靈活的架構。
- 資料庫連線：創建一個連線到資料庫的程式，以便在 chatbot 和資料庫之間進行數據交換。你可以使用程式語言（如Python、Node.js、Java等）的資料庫連線函式庫來實現這個功能。
- 對話紀錄儲存：當 chatbot 與使用者進行對話時，將對話紀錄插入到 "conversations" 資料表中。每次使用者發送新訊息或 chatbot 回覆時，都應該儲存一條新的對話記錄。
- 對話檢索：當需要查看或分析對話紀錄時，你可以編寫查詢來從資料庫中檢索資料。例如，你可以查詢特定使用者的對話歷史，或分析 chatbot 的回覆效能。
- 安全性：注意保護資料庫中的對話資料，特別是如果它包含敏感信息。使用適當的安全措施來確保只有授權的人員能夠存取這些資料。
- 維護：定期備份和維護資料庫，以確保資料的完整性和可用性。

以上是建立一個基本的 chatbot 對話紀錄資料庫的一些步驟和考慮事項。具體的實作方式將取決於你的應用需求和使用的技術堆疊。

## Terminology

### NER

> 在 AI 编程领域，"NER" 代表命名实体识别（Named Entity Recognition）。NER 是自然语言处理（NLP）的一个重要子任务，其主要目标是从文本中识别和提取出具有特定命名的实体，例如人名、地名、组织机构名、日期、货币、百分比等等。

> NER 对于文本分析和信息提取非常有用，它可以帮助计算机理解文本中的上下文，并将文本中>的实体关联到现实世界中的实体。例如，在一份新闻文章中，NER 可以帮助识别报道中提到的人物、地点和日期，从而更好地理解文章的内容。NER 在搜索引擎、信息检索、问答系统、机器翻译等应用中都有广泛的应用。

> NER 通常使用机器学习技术，特别是基于深度学习的方法，来训练模型以自动识别文本中的命名实体。这些模型会学习从上下文中识别实体的模式和规则，并在文本中标注出这些实体的边界。NER 是自然语言处理中的一个关键任务，对于构建智能文本处理系统具有重要意义。

### ALBERT

> 在AI编程领域，"ALBERT" 是一个缩写，代表"A Lite BERT"。它是Google于2019年推出的一种自然语言处理（NLP）模型，是BERT（Bidirectional Encoder Representations from Transformers）的一种轻量级变体。

> BERT是一种革命性的NLP模型，它在各种自然语言处理任务上取得了令人瞩目的成绩。然而，BERT的模型规模很大，需要大量的计算资源来进行训练和部署，这对于许多应用来说可能是不切实际的。

> ALBERT的目标是减小模型的尺寸，同时保持类似BERT的性能。它通过一系列的优化技术来达到这一目标，包括共享参数、嵌入层参数压缩等。ALBERT的精简模型使其更易于训练和部署，同时在多个NLP任务上表现出与大型BERT模型相当的性能。

> 因此，ALBERT是一种旨在提供轻量级但高效的自然语言处理解决方案的NLP模型。它在语义理解、文本分类、命名实体识别等各种NLP任务上都具有广泛的应用。不过，需要注意的是，ALBERT是一个特定的模型，而不是指代某个人或机构的名称。

### chatbot components

> 构建一个完整的domain-know-how的聊天机器人需要哪些工作?

構建一個完整的領域知識聊天機器人需要一系列複雜的工作和組件，包括以下幾個方面：

- 數據收集和準備：
  - 收集領域知識相關的大量文本數據，包括文檔、論文、網頁等。
  - 清理和預處理數據，包括分詞、去除停用詞、詞乾化或詞形還原等。
- 模型選擇：
  - 選擇合適的自然語言處理（NLP）模型，如BERT、GPT、Transformer等，以根據任務需求進行訓練或微調。
  - 考慮是否需要使用**預訓練模型**。
- 知識圖譜和數據庫：
  - 建立領域知識圖譜或數據庫，以存儲結構化的領域知識和數據。
  - 確保知識圖譜或數據庫與NLP模型集成。
- 自然語言理解（NLU）：
  - 開發NLU模塊，將用戶輸入文本解釋為機器可以理解的格式，包括識別意圖、實體識別等。
  - 使用模型或規則來處理用戶輸入。
- 對話管理：
  - 構建對話管理系統，根據用戶的輸入和上下文來決定如何響應。
  - 考慮對話狀態跟踪、會話管理和上下文保持。
- 生成回复(NLG)：
  - 根據NLU的輸出和領域知識，生成自然語言回复。
  - 使用生成式模型（如GPT）或檢索式模型（基於知識圖譜或數據庫的查詢）。
- 評估和優化：
  - 進行離線和在線評估，以確保聊天機器人的性能和用戶滿意度。
  - 根據反饋和性能指標不斷優化模型和系統。
- 部署和整合：
  - 部署聊天機器人到適當的平台，如網站、應用程序、社交媒體等。
  - 整合聊天機器人與其他系統，以獲取實時數據和更新。
- 監控和維護：
  - 設立監控系統，跟踪聊天機器人的性能和用戶反饋。
  - 定期維護和更新模型、知識圖譜和數據庫。
- 用戶界面：
  - 設計用戶友好的界面，使用戶能夠與聊天機器人輕鬆交互。
  - 考慮多渠道支持，如文字聊天、語音、社交媒體等。
- 安全性和隱私：
  - 確保用戶數據的安全性和隱私保護，符合法規要求。
- 培訓和知識維護：
  - 培訓機器人以提供有關領域知識的準確和最新信息。
  - 確保知識的實時性和準確性。

總之，構建一個完整的領域知識聊天機器人是一個複雜的工程，涉及到多個領域，包括自然語言處理、數據庫管理、對話系統設計等。成功的聊天機器人需要持續的工作和不斷的優化，以適應用戶需求和變化的環境。

### Spaces

> 簡要回答"Hugging Face 的Spaces存放的是些甚麼東西?有甚麼功用?"
Hugging Face 的Spaces存放机器学习项目、模型、数据和文档，并提供共享、管理和协作的功能，旨在促进机器学习社区的知识分享和团队协作。

### Prompt Engineer

> "Prompt Engineer"的意義為何?請簡答。

"Prompt Engineer" 意味著一個負責設計和構建自然語言處理（NLP）模型提示（prompts）或查詢的專業人員，這些提示用於與NLP模型進行交互，以產生所需的文本或信息。他們的任務是精心設計提示，以確保模型能夠理解並生成預期的響應。這在各種NLP任務和應用中非常重要，如文本生成、信息檢索、聊天機器人等。

### MLM

> 簡介一下"MLM"是個怎樣的模型。

"MLM" 是 "Masked Language Model"（掩码语言模型）的缩写，是一种自然语言处理（NLP）模型，用于预测文本中被掩盖的词语或标记。MLM 模型的主要特点是在训练时，模型会接受一段文本，其中的某些词语或标记会被随机选择并替换为特殊的掩码标记，然后模型被要求预测这些掩码位置上应该是什么词语。

以下是 MLM 模型的一些关键特点和应用：

语言建模：MLM 模型是一种强大的语言建模工具。通过训练，它可以理解文本的语法、语境和语义，以便在给定上下文的情况下生成合理的文本。

预训练和微调：MLM 模型通常首先在大规模文本语料库上进行预训练，然后在特定任务上进行微调，以适应各种自然语言处理任务，如文本分类、命名实体识别、文本生成等。

上下文理解：MLM 模型能够理解上下文信息，因此在填充掩码位置时可以考虑周围的词语，从而更准确地预测缺失的词语。

生成式任务：MLM 模型也可以用于生成文本，例如文本自动补全、文本摘要、对话生成等任务，其中模型生成文本以填充或扩展给定的上下文。

信息检索：MLM 模型还可以用于信息检索任务，通过对查询进行编码并计算文档的相关性得分。

知名的 MLM 模型包括 BERT（Bidirectional Encoder Representations from Transformers）、RoBERTa（A Robustly Optimized BERT Pretraining Approach）、GPT-2（Generative Pre-trained Transformer 2）等。这些模型在自然语言处理领域取得了令人瞩目的成果，并在各种应用中取得了巨大成功。

### CUDA_VISIBLE_DEVICES

"CUDA_VISIBLE_DEVICES" 是一个用于控制在使用 NVIDIA CUDA 加速的 GPU 程序中哪些 GPU 设备可见的环境变量。这个环境变量的值可以设置为不同的选项，以控制程序使用的 GPU 设备。以下是一些常见的设置值和其含义：

空值或未设置：如果将 "CUDA_VISIBLE_DEVICES" 设置为空值或未设置，那么程序将默认使用所有可用的 GPU 设备。

整数列表：可以将 "CUDA_VISIBLE_DEVICES" 设置为一个整数列表，以指定程序可见的 GPU 设备的索引。例如，如果设置为 "0,2"，程序将只能看到索引为 0 和 2 的 GPU 设备，而其他设备将被隐藏。

GPU 设备名称：也可以将 "CUDA_VISIBLE_DEVICES" 设置为 GPU 设备的名称，以指定程序可见的 GPU 设备。这通常用于在具有命名 GPU 设备的系统上。

"none"：设置 "CUDA_VISIBLE_DEVICES" 为 "none" 将禁用所有 GPU 设备，使程序在 CPU 上运行。

这个环境变量对于在多 GPU 环境中进行深度学习或其他需要 GPU 加速的计算任务非常有用。通过适当设置 "CUDA_VISIBLE_DEVICES"，您可以控制哪些任务分配到哪些 GPU 设备上，从而更有效地管理系统资源。

### Vicuna
 
> 介紹一下vicuna模型 

 Vicuna是一種神經網絡模型，用於自然語言處理。它的特點是使用"soft-label"，即每個單詞都有一個排名，而不是一個 uzc標籤。這使✅模型能夠更好地學習詞彙之間的關聯。Vicuna模型在處理長文本時具有高效率和高準確性。

 ### chatbot

> 如何在既有語言模型中加入領域知識？

要在既有的語言模型中加入特定領域的知識，可以考慮以下方法：

- 資料收集：收集與特定領域相關的大量文本數據，包括文章、報告、文獻、網頁內容等等。這些數據應該盡可能涵蓋該領域的各個方面。

- 預處理數據：對收集到的文本數據進行預處理，包括斷詞、移除停用詞、處理特殊字符等。確保數據質量和一致性。

- 領域特定知識庫：建立一個領域特定的知識庫，其中包含關於該領域的重要知識元素，例如名詞、實體、關鍵詞等。這可以作為模型的參考資源。

- 選擇語言模型：選擇一個強大的語言模型，如GPT-3.5或類似的模型，作為基礎模型。這個模型將用於後續的微調。

- 微調模型：使用收集到的領域相關數據，對基礎模型進行微調。這包括將領域數據加入到訓練過程中，以使模型學習到領域特定的知識。

- 適當的超參數調整：調整微調過程中的超參數，以確保模型的性能最佳化。

- 測試和評估：測試微調後的模型，確保其在領域特定任務上表現良好。可以使用標準測試數據集來評估性能。

- 持續更新：領域知識可能隨時間而變化，因此需要定期更新模型，以保持其在特定領域的準確性和效能。

需要指出的是，微調語言模型需要相當的計算資源和專業知識，因此這可能需要團隊合作和相關的技術支援。此外，確保數據的品質和隱私是非常重要的考慮因素。

### CPU_ISA

> 用中文解釋環境變數"CPU_ISA"的意義為何?

"CPU_ISA"是一個計算機硬體環境變數（Environmental Variable），它代表了處理器的指令集架構（Instruction Set Architecture）。指令集架構是指計算機硬體所支援的指令和操作，它會決定處理器可以執行的指令種類和性能。

例如，IA-32（Intel Architecture 32-bit）指令集架構是一種32位元的指令集架構，它支援Intel處理器的各種指令和操作，包括浮點運算、分支控制、內存管理等。ARM（Advanced RISC Machine）指令集架構是一種基於RISC（Reduced Instruction Set Computing）的指令集架構，它主要用於智能手機、平板電腦和嵌入式系統等裝置中的處理器。

總的來說，CPU_ISA環境變數反映了處理器的指令集架構，它會決定系統和應用程序所能執行的指令種類和性能。

> 

CPU_ISA=amx" 是一種相對較新的指令集架構，特定於某些處理器或架構。AMX（Advanced Matrix Extensions）是英特爾（Intel）引入的一種加速矩陣計算的技術。它旨在提高矩陣計算工作負載的性能，這對於深度學習和其他涉及大規模矩陣操作的計算密集型應用非常有用。

"CPU_ISA=amx" 環境變量通常用於指示操作系統或應用程序，在支持 AMX 技術的特定 CPU 上運行時，可以利用 AMX 指令集來執行矩陣計算任務。這有助於提高計算性能並加速相關的應用程序，特別是在深度學習訓練和推斷等領域。

因此，"CPU_ISA=amx" 通常適用於以下情況：

深度學習：在深度神經網絡的訓練和推斷過程中，矩陣計算佔據了大部分計算工作負載，因此 AMX 技術可以顯著提高性能。

科學計算：對於需要大規模矩陣操作的科學計算應用，如數值模擬、仿真和數據分析，啟用 AMX 可以加速計算。

請注意，要使用 "CPU_ISA=amx"，您需要具備支持 AMX 技術的特定英特爾 CPU，並且操作系統和應用程序也必須能夠識別和利用 AMX 指令集。這通常需要相應的硬件和軟件支持。

### LLM size

> LLM 語言模型中規格有7B、13B、16K等等，指得是甚麼?

在自然语言处理（NLP）领域，LLM 是 "Large Language Model" 的缩写，指的是大型语言模型。规格中的数字通常表示模型的大小和参数数量。以下是一些常见规格的解释：

7B：这表示模型的参数数量约为 7 亿（Billion，十亿）。这是一个相对较大的语言模型，可以用于各种文本生成和理解任务。

13B：这表示模型的参数数量约为 13 亿。比 7B 模型更大，具有更多的参数，因此在某些任务上可能表现更好。

16K：这表示模型的大小约为 16 千兆字节（Gigabytes，GB）。这种规格通常用于描述模型的内存占用或模型文件的大小。

这些规格中的数字越大，通常表示模型越大、更强大，但也需要更多的计算资源来训练和运行。选择适当规格的模型取决于任务需求、可用的硬件资源和性能要求。在使用大型语言模型时，还需要考虑到训练和推理的时间和成本。

### document analysis to structure data storage

> 如何記錄"document analysis"的結構化數據成果?

要記錄 "document analysis" 的結構化數據成果，通常可以採取以下步驟：

1. 確定數據結構：首先，確定要記錄的結構化數據的類型和格式。這可能包括文本、數字、日期、**實體關係**等不同類型的數據。
2. 選擇存儲方法：選擇合適的數據存儲方法，這取決於數據量和訪問需求。常見的選擇包括數據庫、數據倉庫、**知識圖譜**等。
3. 定義數據模型：創建一個數據模型或數據架構，以描述數據的結構和關係。這可以使用數據庫表格、**圖譜**節點和邊緣、**JSON 架構**等方式來實現。
4. 數據提取和轉換：使用適當的工具和技術，從文檔中提取結構化數據。這可能涉及文本分析、自然語言處理、實體識別、關鍵詞提取等任務。
5. 數據存儲：將提取的結構化數據存儲到選擇的數據存儲系統中。確保數據按照定義的數據模型進行組織和存儲。
6. 數據索引和檢索：為了能夠有效地檢索和查詢數據，創建適當的索引和查詢接口。這可以幫助用戶快速訪問所需的信息。
7. 數據質量和驗證：確保數據的準確性和一致性。進行數據驗證和清洗，以去除錯誤或不完整的數據。
8. 文檔關聯：將結構化數據與原始文檔相關聯，以便隨時查看原始文檔並跟踪數據的來源。
9. 權限和訪問控制：根據需要實施數據的訪問控制和權限管理，以確保數據的安全性和隱私保護。
10. 文檔記錄：記錄有關數據提取、轉換和存儲的元數據，包括處理日期、處理人員等信息。
11. 備份和恢復：定期備份數據，以防止數據丟失或損壞。確保有恢復機制。
12. 文檔化：為數據結構和處理過程創建文檔，以便用戶和團隊了解數據的含義和處理方式。
13. 監控和維護：建立監控機制，以及時檢測和解決數據質量問題。定期維護和更新數據。

以上步驟有助於確保 "document analysis" 的結構化數據成果得以有效記錄、管理和維護，以滿足各種分析和應用的需求。

### nodes and edges
 
> 將引號內文字寫成圖譜*節點和邊緣、並以JSON 架構方式來表達。

"近日業主上級單位對本部經辦之某下水道工程，進行施工品質查核時，因業主、監造(本公司)及承包商之簡報，一再重複強調工程進度落後為既有地下管線遷移無法配合，並提出解決辦法及建議等，但查核委員認為本公司同時負責設計及監造，管線遷移問題亦屬本公司之責任範圍，導致本公司需解釋服務契約之工作範圍、設計工作如何進行等，以排除本公司之責任，如查核委員不能接受解釋時，將會影響查核分數(事實上，該次查核共有二標，經過延長工期，調整預定進度後，前一標進度為超前，後一標進度雖落後，但經本公司監造計畫要求提出趕工計畫，積極執行後，進度落後已減少，可望在若干時日後不再落後)。"

```json
{
  "nodes": [
    {
      "id": "業主",
      "type": "人物"
    },
    {
      "id": "監造(本公司)",
      "type": "组织"
    },
    {
      "id": "承包商",
      "type": "组织"
    },
    {
      "id": "查核委員",
      "type": "人物"
    }
  ],
  "edges": [
    {
      "source": "業主",
      "target": "監造(本公司)",
      "relation": "上级单位"
    },
    {
      "source": "監造(本公司)",
      "target": "承包商",
      "relation": "承包关系"
    },
    {
      "source": "查核委員",
      "target": "監造(本公司)",
      "relation": "查核关系"
    },
    {
      "source": "查核委員",
      "target": "業主",
      "relation": "查核关系"
    }
  ],
  "text": "近日業主上級單位對本部經辦之某下水道工程，進行施工品質查核时..."
}
```
