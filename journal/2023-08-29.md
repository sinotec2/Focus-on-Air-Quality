# Tuesday, August 29, 2023 NLP

## Definition

### amazon

- [什麼是自然語言處理 (NLP)？](https://aws.amazon.com/tw/what-is/nlp/)
  > 自然語言處理 (NLP) 是一種機器學習技術，讓電腦能夠解譯、操縱及理解人類語言。如今，組織擁有來自各種通訊管道的大量語音和文字資料，例如電子郵件、簡訊、社交媒體新聞摘要、影片、音訊等。他們使用 NLP 軟體來自動處理此資料，分析訊息中的意圖或情緒，並即時回應人類通訊。
  - 為什麼 NLP 很重要？
  - 什麼是企業的 NLP 使用案例？
  - NLP 如何運作？
  - 什麼是 NLP 任務？
  - 自然語言處理採用哪些方法？
  > 針對想要在其業務中建立標準 NLP 解決方案的客戶，Amazon SageMaker 可透過全受管基礎設施、工具和工作流程，輕鬆準備資料，以及建置、訓練和部署機器學習模型，包括適用於商業分析師的無程式碼服務。藉助 Amazon SageMaker 上的 *Hugging Face*，您可以部署和微調來自 *Hugging Face* 的預先訓練模型，*Hugging Face* 是稱為 Transformers 的自然語言處理 (NLP) 模型的開放原始碼供應商，將設定和使用這些 NLP 模型所需的時間從數週縮短至幾分鐘。

立即建立 AWS 帳戶，開始使用自然語言處理 (NLP)。

## Hugging Face

- The AI community building the future.
- The platform where the machine learning community collaborates on models, datasets, and applications.
- If you need to create a repo from the command line (skip if you created a repo from the website)
- HuggingGPT爆紅，Hugging Face又是什麼？它正在拆掉OpenAI的圍牆，要當AI界的Github、[36Kr 發表於 2023年4月30日 16:00 電腦王->](https://www.techbang.com/posts/105484-hugginggpt-is-on-fire-what-is-hugging-face-hugging-face-a-2)
  >  
- Hugging Face 架構與三大神器 by [大魔術熊貓工程師2022-09-17 20:35:25@ithelp.ithome](https://ithelp.ithome.com.tw/articles/10291757)

### Getting started

- with our git and git-lfs interface

```bash
pip install huggingface_hub
#You already have it if you installed transformers or datasets

huggingface-cli login

# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}
```

### Clone your model or dataset locally

```bash
#Make sure you have git-lfs installed
#(https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name
```

- Then add, commit and push any file you want, including larges files

```bash
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push
```

- In most cases, if you're using one of the compatible libraries, your repo will then be accessible from code, through its identifier: username/repo_name

For example for a transformers model, anyone can load it with:

```python
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```

### 中研院詞語庫

- [CKIP Lab](https://huggingface.co/ckiplab)
  1. 繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）[bert-base-chinese-ner](https://huggingface.co/ckiplab/bert-base-chinese-ner)
  2. (same as above, fill-mask and lm-head)[bert-base-chinese](https://huggingface.co/ckiplab/bert-base-chinese)
  3. [albert-tiny-chinese-ws](https://huggingface.co/ckiplab/albert-tiny-chinese-ws)

## Pile

- The Pile: An 800GB Dataset of Diverse Text for Language Modeling @[eleuther(2020)](https://pile.eleuther.ai/)
- `https://the-eye.eu/public/AI/pile_preliminary_components`目錄已經不存在了，改成`https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/`

## training parameters

### training arguments

```python
batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
```

- day23

```python
args = Seq2SeqTrainingArguments( 
    output_dir=f"{model_name}-finetuned", 
    num_train_epochs=1, 
    warmup_steps=100,
    per_device_train_batch_size=10, 
    per_device_eval_batch_size=10,
    weight_decay=0.01, 
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=100, 
    save_steps=1e6,
    gradient_accumulation_steps=64,
    report_to="azure_ml"
)
```

### compute_metrics

- day16

```python
from sklearn.metrics import accuracy_score, f1_score
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}
```

- day23

```python
from datasets import load_metric
rouge_metric = load_metric("rouge")
def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # 這裡把 DataCollatorForSeq2Seq 會填入的 -100 排除掉
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]

    result = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

### trainer settings

- day16

```python
```

- day23

```python
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset= dataset_pt["train"],
    eval_dataset = dataset_pt["valid"],
    data_collator=seq2seq_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```

## ChatPDF

- 官網：https://www.chatpdf.com/
- 

## Terminology

### NER

> 在 AI 编程领域，"NER" 代表命名实体识别（Named Entity Recognition）。NER 是自然语言处理（NLP）的一个重要子任务，其主要目标是从文本中识别和提取出具有特定命名的实体，例如人名、地名、组织机构名、日期、货币、百分比等等。

> NER 对于文本分析和信息提取非常有用，它可以帮助计算机理解文本中的上下文，并将文本中>的实体关联到现实世界中的实体。例如，在一份新闻文章中，NER 可以帮助识别报道中提到的人物、地点和日期，从而更好地理解文章的内容。NER 在搜索引擎、信息检索、问答系统、机器翻译等应用中都有广泛的应用。

> NER 通常使用机器学习技术，特别是基于深度学习的方法，来训练模型以自动识别文本中的命名实体。这些模型会学习从上下文中识别实体的模式和规则，并在文本中标注出这些实体的边界。NER 是自然语言处理中的一个关键任务，对于构建智能文本处理系统具有重要意义。

### ALBERT 

> 在AI编程领域，"ALBERT" 是一个缩写，代表"A Lite BERT"。它是Google于2019年推出的一种自然语言处理（NLP）模型，是BERT（Bidirectional Encoder Representations from Transformers）的一种轻量级变体。

> BERT是一种革命性的NLP模型，它在各种自然语言处理任务上取得了令人瞩目的成绩。然而，BERT的模型规模很大，需要大量的计算资源来进行训练和部署，这对于许多应用来说可能是不切实际的。

> ALBERT的目标是减小模型的尺寸，同时保持类似BERT的性能。它通过一系列的优化技术来达到这一目标，包括共享参数、嵌入层参数压缩等。ALBERT的精简模型使其更易于训练和部署，同时在多个NLP任务上表现出与大型BERT模型相当的性能。

> 因此，ALBERT是一种旨在提供轻量级但高效的自然语言处理解决方案的NLP模型。它在语义理解、文本分类、命名实体识别等各种NLP任务上都具有广泛的应用。不过，需要注意的是，ALBERT是一个特定的模型，而不是指代某个人或机构的名称。
>
 