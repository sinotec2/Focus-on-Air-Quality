# Tuesday, August 29, 2023 NLP

## Definition

### amazon

- [什麼是自然語言處理 (NLP)？](https://aws.amazon.com/tw/what-is/nlp/)
  > 自然語言處理 (NLP) 是一種機器學習技術，讓電腦能夠解譯、操縱及理解人類語言。如今，組織擁有來自各種通訊管道的大量語音和文字資料，例如電子郵件、簡訊、社交媒體新聞摘要、影片、音訊等。他們使用 NLP 軟體來自動處理此資料，分析訊息中的意圖或情緒，並即時回應人類通訊。
  - 為什麼 NLP 很重要？
  - 什麼是企業的 NLP 使用案例？
  - NLP 如何運作？
  - 什麼是 NLP 任務？
  - 自然語言處理採用哪些方法？
  > 針對想要在其業務中建立標準 NLP 解決方案的客戶，Amazon SageMaker 可透過全受管基礎設施、工具和工作流程，輕鬆準備資料，以及建置、訓練和部署機器學習模型，包括適用於商業分析師的無程式碼服務。藉助 Amazon SageMaker 上的 *Hugging Face*，您可以部署和微調來自 *Hugging Face* 的預先訓練模型，*Hugging Face* 是稱為 Transformers 的自然語言處理 (NLP) 模型的開放原始碼供應商，將設定和使用這些 NLP 模型所需的時間從數週縮短至幾分鐘。

立即建立 AWS 帳戶，開始使用自然語言處理 (NLP)。

## Hugging Face

- The AI community building the future.
- The platform where the machine learning community collaborates on models, datasets, and applications.
- If you need to create a repo from the command line (skip if you created a repo from the website)
- HuggingGPT爆紅，Hugging Face又是什麼？它正在拆掉OpenAI的圍牆，要當AI界的Github、[36Kr 發表於 2023年4月30日 16:00 電腦王->](https://www.techbang.com/posts/105484-hugginggpt-is-on-fire-what-is-hugging-face-hugging-face-a-2)
  >  
- Hugging Face 架構與三大神器 by [大魔術熊貓工程師2022-09-17 20:35:25@ithelp.ithome](https://ithelp.ithome.com.tw/articles/10291757)

### Getting started

- with our git and git-lfs interface

```bash
pip install huggingface_hub
#You already have it if you installed transformers or datasets

huggingface-cli login

# Log in using a token from huggingface.co/settings/tokens
# Create a model or dataset repo from the CLI if needed
huggingface-cli repo create repo_name --type {model, dataset, space}
```

### Clone your model or dataset locally

```bash
#Make sure you have git-lfs installed
#(https://git-lfs.github.com)
git lfs install
git clone https://huggingface.co/username/repo_name
```

- Then add, commit and push any file you want, including larges files

```bash
# save files via `.save_pretrained()` or move them here
git add .
git commit -m "commit from $USER"
git push
```

- In most cases, if you're using one of the compatible libraries, your repo will then be accessible from code, through its identifier: username/repo_name

For example for a transformers model, anyone can load it with:

```python
tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")
```

### 中研院詞語庫

- [CKIP Lab](https://huggingface.co/ckiplab)
  1. 繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）[bert-base-chinese-ner](https://huggingface.co/ckiplab/bert-base-chinese-ner)
  2. (same as above, fill-mask and lm-head)[bert-base-chinese](https://huggingface.co/ckiplab/bert-base-chinese)
  3. [albert-tiny-chinese-ws](https://huggingface.co/ckiplab/albert-tiny-chinese-ws)

### 法律相關

- 宝锣法律大模型1.0 [xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b](https://huggingface.co/xuanxuanzl/BaoLuo-LawAssistant-sftglm-6b)
  - 綁定使用GPU
- LawBERT
  - Dataset Card for "LawBERT_data" [LawBERT-tw/LawBERT_data ](https://huggingface.co/datasets/LawBERT-tw/LawBERT_data)
  - model: [LawBERT-s](https://huggingface.co/LawBERT-tw/LawBERT-s)
    - BertForMaskedLM
- JessyTsu1
  - Model:[ChatLaw-13B](https://huggingface.co/JessyTsu1/ChatLaw-13B)

### 工程相關

Data comes from Published Texas A&M Engineering News and was used to train a MLM @3epochs 500([houck2040/engineering](https://huggingface.co/datasets/houck2040/engineering))

### 環境相關

- Korean Dataset Construction for Relation Extraction Task on Natures and Environments 韓語詞彙關係 ([kimcando/KOR-RE-natures-and-environments](https://huggingface.co/datasets/kimcando/KOR-RE-natures-and-environments))
- [climatebert/environmental_claims](https://huggingface.co/datasets/climatebert/environmental_claims)
  - label: the label (0 -> no environmental claim, 1 -> environmental claim)

### jondurbin

- [jondurbin/airoboros-13b-gpt4-1.4](https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4)有30多萬次下載，是gpt4.1.4最新的延伸版本

### FastChat

這個界面獲得2萬多個github好評。會需要3個埠(不能重複。程式內設都會是21001，只能用一次)
1. 控制埠($addc=http://${ip}:21001內設)，搭配controller-address(作業埠需要)/controller-url(UI埠需要)之引數
2. 作業埠($add=http://${ip}:$port)。引數worker-address只需設定一次。
3. UI埠($addu=http://${ip}:$port1)，為使用者最後看到的界面。

安裝與其餘設定詳見[jondurbin/FastChat](https://github.com/jondurbin/FastChat)。此處詳解devp(ip=200.200.32.195)上的啟動方式。([結果位址](http://200.200.32.195:55081))


- 控制埠
  - 如不指定host，則會是內設值localhost。

```bash
python3 -m fastchat.serve.controller --host $ip &
```

- 模型作業埠
  - model-path可以指定到特定的模型。不一定是lmsys。
  - 上接控制埠、下啟UI埠。
  - 同一個控制埠及UI埠可以(同時)使用多個模型，只要開在不同的位址就好了。
  - 必須指定device，否則內設為gpu


```bash
python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5-16k \
--host $ip \
--worker-address $add \
--controller-address $addc --port $port --device cpu &
```

- 測試模型是否正常運作
  - 需指定工作埠位址。啟動模型也必須與工作模型一致。
  - 此處不需再指定device

```bash
python3 -m fastchat.serve.test_message --model-name vicuna-7b-v1.5-16k --worker-address $add
```

- webui程式埠
  - 需要設定2組位址，一個是控制埠(controller-url)，一個是使用者會連上的最後位址。

```bash
python3 -m fastchat.serve.gradio_web_server --host $ip --port $port1 --controller-url $addc
```

## Pile

- The Pile: An 800GB Dataset of Diverse Text for Language Modeling @[eleuther(2020)](https://pile.eleuther.ai/)
- `https://the-eye.eu/public/AI/pile_preliminary_components`目錄已經不存在了，改成`https://the-eye.eu/public/AI/training_data/code_clippy_data/code_clippy_dup_data/train/`

## training parameters

### training arguments

```python
batch_size = 64
logging_steps = len(sentiment_encoded["train"]) // batch_size
model_name = "poem_model"
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=40,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  disable_tqdm=False,
                                  label_names= labels,
                                  report_to = "azure_ml",
                                  logging_steps=logging_steps)
```

- day23

```python
args = Seq2SeqTrainingArguments( 
    output_dir=f"{model_name}-finetuned", 
    num_train_epochs=1, 
    warmup_steps=100,
    per_device_train_batch_size=10, 
    per_device_eval_batch_size=10,
    weight_decay=0.01, 
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=100, 
    save_steps=1e6,
    gradient_accumulation_steps=64,
    report_to="azure_ml"
)
```

### compute_metrics

- day16

```python
from sklearn.metrics import accuracy_score, f1_score
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}
```

- day23

```python
from datasets import load_metric
rouge_metric = load_metric("rouge")
def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # 這裡把 DataCollatorForSeq2Seq 會填入的 -100 排除掉
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]

    result = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}
```

### trainer settings

- day16

```python
```

- day23

```python
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset= dataset_pt["train"],
    eval_dataset = dataset_pt["valid"],
    data_collator=seq2seq_data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```

## pipeline tasks


available tasks are 
- 'audio-classification', 
- 'automatic-speech-recognition', 
- 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 
- 'fill-mask', 
- 'image-classification', 'image-segmentation', 'image-to-text', 
- 'mask-generation', 'ner', 'object-detection', 
- 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 
- 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 
- 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 
- 'zero-shot-audio-classification', 'zero-shot-classification', 
- 'zero-shot-image-classification', 'zero-shot-object-detection', 
- 'translation_XX_to_YY'

## ChatPDF

- 官網：[ttps://www.chatpdf.com](ttps://www.chatpdf.com)

Free|Plus
-|-
$0/mo|$5/mo
120 pages/PDF|2,000 pages/PDF
10 MB/PDF|32 MB/PDF
3 PDFs/day|50 PDFs/day
50 questions/day|1000 questions/day

## Terminology

### NER

> 在 AI 编程领域，"NER" 代表命名实体识别（Named Entity Recognition）。NER 是自然语言处理（NLP）的一个重要子任务，其主要目标是从文本中识别和提取出具有特定命名的实体，例如人名、地名、组织机构名、日期、货币、百分比等等。

> NER 对于文本分析和信息提取非常有用，它可以帮助计算机理解文本中的上下文，并将文本中>的实体关联到现实世界中的实体。例如，在一份新闻文章中，NER 可以帮助识别报道中提到的人物、地点和日期，从而更好地理解文章的内容。NER 在搜索引擎、信息检索、问答系统、机器翻译等应用中都有广泛的应用。

> NER 通常使用机器学习技术，特别是基于深度学习的方法，来训练模型以自动识别文本中的命名实体。这些模型会学习从上下文中识别实体的模式和规则，并在文本中标注出这些实体的边界。NER 是自然语言处理中的一个关键任务，对于构建智能文本处理系统具有重要意义。

### ALBERT

> 在AI编程领域，"ALBERT" 是一个缩写，代表"A Lite BERT"。它是Google于2019年推出的一种自然语言处理（NLP）模型，是BERT（Bidirectional Encoder Representations from Transformers）的一种轻量级变体。

> BERT是一种革命性的NLP模型，它在各种自然语言处理任务上取得了令人瞩目的成绩。然而，BERT的模型规模很大，需要大量的计算资源来进行训练和部署，这对于许多应用来说可能是不切实际的。

> ALBERT的目标是减小模型的尺寸，同时保持类似BERT的性能。它通过一系列的优化技术来达到这一目标，包括共享参数、嵌入层参数压缩等。ALBERT的精简模型使其更易于训练和部署，同时在多个NLP任务上表现出与大型BERT模型相当的性能。

> 因此，ALBERT是一种旨在提供轻量级但高效的自然语言处理解决方案的NLP模型。它在语义理解、文本分类、命名实体识别等各种NLP任务上都具有广泛的应用。不过，需要注意的是，ALBERT是一个特定的模型，而不是指代某个人或机构的名称。

### chatbot components

> 构建一个完整的domain-know-how的聊天机器人需要哪些工作?

構建一個完整的領域知識聊天機器人需要一系列複雜的工作和組件，包括以下幾個方面：

- 數據收集和準備：
  - 收集領域知識相關的大量文本數據，包括文檔、論文、網頁等。
  - 清理和預處理數據，包括分詞、去除停用詞、詞乾化或詞形還原等。
- 模型選擇：
  - 選擇合適的自然語言處理（NLP）模型，如BERT、GPT、Transformer等，以根據任務需求進行訓練或微調。
  - 考慮是否需要使用**預訓練模型**。
- 知識圖譜和數據庫：
  - 建立領域知識圖譜或數據庫，以存儲結構化的領域知識和數據。
  - 確保知識圖譜或數據庫與NLP模型集成。
- 自然語言理解（NLU）：
  - 開發NLU模塊，將用戶輸入文本解釋為機器可以理解的格式，包括識別意圖、實體識別等。
  - 使用模型或規則來處理用戶輸入。
- 對話管理：
  - 構建對話管理系統，根據用戶的輸入和上下文來決定如何響應。
  - 考慮對話狀態跟踪、會話管理和上下文保持。
- 生成回复(NLG)：
  - 根據NLU的輸出和領域知識，生成自然語言回复。
  - 使用生成式模型（如GPT）或檢索式模型（基於知識圖譜或數據庫的查詢）。
- 評估和優化：
  - 進行離線和在線評估，以確保聊天機器人的性能和用戶滿意度。
  - 根據反饋和性能指標不斷優化模型和系統。
- 部署和整合：
  - 部署聊天機器人到適當的平台，如網站、應用程序、社交媒體等。
  - 整合聊天機器人與其他系統，以獲取實時數據和更新。
- 監控和維護：
  - 設立監控系統，跟踪聊天機器人的性能和用戶反饋。
  - 定期維護和更新模型、知識圖譜和數據庫。
- 用戶界面：
  - 設計用戶友好的界面，使用戶能夠與聊天機器人輕鬆交互。
  - 考慮多渠道支持，如文字聊天、語音、社交媒體等。
- 安全性和隱私：
  - 確保用戶數據的安全性和隱私保護，符合法規要求。
- 培訓和知識維護：
  - 培訓機器人以提供有關領域知識的準確和最新信息。
  - 確保知識的實時性和準確性。

總之，構建一個完整的領域知識聊天機器人是一個複雜的工程，涉及到多個領域，包括自然語言處理、數據庫管理、對話系統設計等。成功的聊天機器人需要持續的工作和不斷的優化，以適應用戶需求和變化的環境。

### Spaces

> 簡要回答"Hugging Face 的Spaces存放的是些甚麼東西?有甚麼功用?"
Hugging Face 的Spaces存放机器学习项目、模型、数据和文档，并提供共享、管理和协作的功能，旨在促进机器学习社区的知识分享和团队协作。

### Prompt Engineer

> "Prompt Engineer"的意義為何?請簡答。

"Prompt Engineer" 意味著一個負責設計和構建自然語言處理（NLP）模型提示（prompts）或查詢的專業人員，這些提示用於與NLP模型進行交互，以產生所需的文本或信息。他們的任務是精心設計提示，以確保模型能夠理解並生成預期的響應。這在各種NLP任務和應用中非常重要，如文本生成、信息檢索、聊天機器人等。

### MLM

> 簡介一下"MLM"是個怎樣的模型。

"MLM" 是 "Masked Language Model"（掩码语言模型）的缩写，是一种自然语言处理（NLP）模型，用于预测文本中被掩盖的词语或标记。MLM 模型的主要特点是在训练时，模型会接受一段文本，其中的某些词语或标记会被随机选择并替换为特殊的掩码标记，然后模型被要求预测这些掩码位置上应该是什么词语。

以下是 MLM 模型的一些关键特点和应用：

语言建模：MLM 模型是一种强大的语言建模工具。通过训练，它可以理解文本的语法、语境和语义，以便在给定上下文的情况下生成合理的文本。

预训练和微调：MLM 模型通常首先在大规模文本语料库上进行预训练，然后在特定任务上进行微调，以适应各种自然语言处理任务，如文本分类、命名实体识别、文本生成等。

上下文理解：MLM 模型能够理解上下文信息，因此在填充掩码位置时可以考虑周围的词语，从而更准确地预测缺失的词语。

生成式任务：MLM 模型也可以用于生成文本，例如文本自动补全、文本摘要、对话生成等任务，其中模型生成文本以填充或扩展给定的上下文。

信息检索：MLM 模型还可以用于信息检索任务，通过对查询进行编码并计算文档的相关性得分。

知名的 MLM 模型包括 BERT（Bidirectional Encoder Representations from Transformers）、RoBERTa（A Robustly Optimized BERT Pretraining Approach）、GPT-2（Generative Pre-trained Transformer 2）等。这些模型在自然语言处理领域取得了令人瞩目的成果，并在各种应用中取得了巨大成功。

### CUDA_VISIBLE_DEVICES

"CUDA_VISIBLE_DEVICES" 是一个用于控制在使用 NVIDIA CUDA 加速的 GPU 程序中哪些 GPU 设备可见的环境变量。这个环境变量的值可以设置为不同的选项，以控制程序使用的 GPU 设备。以下是一些常见的设置值和其含义：

空值或未设置：如果将 "CUDA_VISIBLE_DEVICES" 设置为空值或未设置，那么程序将默认使用所有可用的 GPU 设备。

整数列表：可以将 "CUDA_VISIBLE_DEVICES" 设置为一个整数列表，以指定程序可见的 GPU 设备的索引。例如，如果设置为 "0,2"，程序将只能看到索引为 0 和 2 的 GPU 设备，而其他设备将被隐藏。

GPU 设备名称：也可以将 "CUDA_VISIBLE_DEVICES" 设置为 GPU 设备的名称，以指定程序可见的 GPU 设备。这通常用于在具有命名 GPU 设备的系统上。

"none"：设置 "CUDA_VISIBLE_DEVICES" 为 "none" 将禁用所有 GPU 设备，使程序在 CPU 上运行。

这个环境变量对于在多 GPU 环境中进行深度学习或其他需要 GPU 加速的计算任务非常有用。通过适当设置 "CUDA_VISIBLE_DEVICES"，您可以控制哪些任务分配到哪些 GPU 设备上，从而更有效地管理系统资源。

### Vicuna
 
> 介紹一下vicuna模型 

 Vicuna是一種神經網絡模型，用於自然語言處理。它的特點是使用"soft-label"，即每個單詞都有一個排名，而不是一個 uzc標籤。這使✅模型能夠更好地學習詞彙之間的關聯。Vicuna模型在處理長文本時具有高效率和高準確性。

 ### chatbot

> 如何在既有語言模型中加入領域知識？

要在既有的語言模型中加入特定領域的知識，可以考慮以下方法：

- 資料收集：收集與特定領域相關的大量文本數據，包括文章、報告、文獻、網頁內容等等。這些數據應該盡可能涵蓋該領域的各個方面。

- 預處理數據：對收集到的文本數據進行預處理，包括斷詞、移除停用詞、處理特殊字符等。確保數據質量和一致性。

- 領域特定知識庫：建立一個領域特定的知識庫，其中包含關於該領域的重要知識元素，例如名詞、實體、關鍵詞等。這可以作為模型的參考資源。

- 選擇語言模型：選擇一個強大的語言模型，如GPT-3.5或類似的模型，作為基礎模型。這個模型將用於後續的微調。

- 微調模型：使用收集到的領域相關數據，對基礎模型進行微調。這包括將領域數據加入到訓練過程中，以使模型學習到領域特定的知識。

- 適當的超參數調整：調整微調過程中的超參數，以確保模型的性能最佳化。

- 測試和評估：測試微調後的模型，確保其在領域特定任務上表現良好。可以使用標準測試數據集來評估性能。

- 持續更新：領域知識可能隨時間而變化，因此需要定期更新模型，以保持其在特定領域的準確性和效能。

需要指出的是，微調語言模型需要相當的計算資源和專業知識，因此這可能需要團隊合作和相關的技術支援。此外，確保數據的品質和隱私是非常重要的考慮因素。
