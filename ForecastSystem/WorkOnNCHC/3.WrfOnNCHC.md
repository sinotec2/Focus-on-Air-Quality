---
layout: default
title: 國網上執行wrf專案
parent: Works on NCHC
grand_parent: Forecast Systems
nav_order: 3
date: 2023-03-17
last_modified_date: 2023-03-21 20:50:14
tags: forecast CMAQ nchc_service m3nc2gif
---

# 國網上執行wrf專案

{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>
---

## 背景

- 這部分有很大的差異，主要是real(ndown)及wrf需要平行計算，國網在此表顯非常優秀。除此之外，WPS及後處理則沒有使用[slurm][slurm]。
- wrfout檔案[格式轉換][trans]及[序列版mcip](#序列運作方案)包裝在一起，放在背景執行，不影響程序的進行。

## 程式之編譯

- 理論上有這麼多人在[國網][nchc]上的執行經驗了，但似乎沒有開一個特定的Slurm Module給wrf的使用者，使用者(或者是研究群)還是必須就自己的需求、版本、模式的上下銜接等等，來編譯自己的wrf程式。定義此處的需求：
  - 讀取GFS預報結果進行wrf FDDA計算
  - 需與CMAQ系統的mcip相連
  - 需輸出U10、V10與HPBL以利地面軌跡之計算

### WRF 版本的選擇

- CMAQ/MCIP(5.1)系統目前還能接受WRF4.0、WRF4.2等版本
- 與[trans_wrfout.py][trans]模板的版本有關，二者需一致化。

### PNETCDF的應用及問題

- 國網上的netcdf預設是會啟動平行IO的(intel MPI或其他版本)、也就是會需要連結PNETCDF的。無一例外。這點可以搜尋所有`libnetcdf.settings`的內容加以證實。
- 目前國網上共有7個版本的netcdf程式庫如下，分別是4.7.4與4.8.1，使用的編譯程式有intel版本(2020~2021)，也有gcc版本，mpi也有openmpi(405~410)及intelmpi。其中除了gcc版本(單機版、沒有`libnetcdff.a`)外，其餘皆已開啟pnetcdf設定。

```bash
sinotec2@lgn303 /opt/ohpc/Taiwania3/libs
$ findc libnetcdf.settings
./i2020-Ompi405/netcdf-4.7.4/lib/libnetcdf.settings
./Iimpi-2021/netcdf-4.7.4/lib/libnetcdf.settings
./i2020-Ompi410/netcdf-4.7.4/lib64/libnetcdf.settings
./i2021-Ompi410/netcdf-4.7.4/lib/libnetcdf.settings
./i2021-Ompi405/netcdf-4.7.4/lib/libnetcdf.settings
./gcc-8.3.0/netcdf/4.8.1/lib64/libnetcdf.settings
./Iimpi-2020/netcdf-4.7.4/lib/libnetcdf.settings
```

- PNETCDF面對最大的問題是mcip的平行化非常容易錯誤。一般也認為mcip沒有必要執行平行化，因此銜接上出現問題。這個問題在討論到mcip時會詳細說明。

### wrf configure選項

- 依據北卡州立大學高速電腦中心的[經驗](https://hpc.ncsu.edu/Software/Software.php)，wrf configure選項設定為15。並沒有針對GPU的特性另外設定。
- 經建議[^1]嘗試選擇20 (INTEL (ifort/icc): Xeon (SNB with AVX mods))，執行不是很順利。

```bash
module load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2
./configure
./compile -j 4 em_real
```

### WPS之編譯

- 會需要zlib、png、jasper
- 此處按照[Pratiman 2020][Pratiman]的建議，自行下載原始碼來編譯。

## WPS

### gfs數據下載與處理

- 國網下載很快，不需要平行運作，依序進行即可。為方便執行，另建一個批次檔get_gfs.cs。

```bash
#sinotec2@lgn303 /work/sinotec2/WRF4/WRF4.2.1
#$ cat  $gfs/get_gfs.cs
wget=/usr/bin/wget
root=https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.
BH=00
dir=$begdp/$BH/atmos/

cd $gfs

for ((i=0;i <= 312; i+=3));do
  iii=$(printf "%03d" $i)
  file=gfs.t${BH}z.pgrb2.1p00.f$iii
  if [ -e $file ];then rm $file;fi
  $wget --no-check-certificate -q --retry-connrefused --waitretry=3 --random-wait \
        --read-timeout=20 --timeout=15 -t 10 --continue $root$dir$file
done
```

- 序列處理，因此置換日期是全段模擬的起迄日，不必再每個timeframe分開做(參[])。

```bash
./link_grib.csh gfs*
hh=00
cp namelist.wps_loop namelist.wps
for cmd in 's/BEGD/'$BEGD'/g' 's/ENDD/'$ENDD'/g' 's/HH/'$hh'/g';do sed -ie $cmd namelist.wps;done
../UGB
```

- wps系列程式的編譯是用ifort，外加自行編譯的jasper與png，因此UGB的內容也有所不同。

```bash
module purge
module load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/work/sinotec2/opt/jasper/lib:/work/sinotec2/opt/png/lib ./ungrib.exe
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/work/sinotec2/opt/jasper/lib:/work/sinotec2/opt/png/lib ./metgrid.exe
```

### real及ndown

- 使用3個node，每個node上執行40個程式，因此總程序為120
- slurm設定方式參考[wrf-on-slurm](https://codelabs.developers.google.com/codelabs/wrf-on-slurm-gcp#3) 、[WRF 软件使用教程](https://ac.sugon.com/doc/1.0.6/11268/general-handbook/software-tutorial/wrf.html)及TWCC - III 使用手冊：[IntelMPI](https://man.twcc.ai/@TWCC-III-manual/H1Vyiuos_)

```bash
sinotec2@lgn303 /work/sinotec2/WRF4
$ cat ./WRF4.2.1/doreal
#!/bin/bash
#SBATCH -A ENT111040             # Account name/project number
#SBATCH -J wrf                   # Job name
#SBATCH -p ct224                 # Partiotion name
#SBATCH -n 120                   # Number of MPI tasks (i.e. processes)
#SBATCH -c 1                     # Number of cores per MPI task
#SBATCH -N 3                     # Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=40     # Maximum number of tasks on each node
#SBATCH -o rsl.out.%j            # Path to the standard output file
#SBATCH -e rsl.error.%j          # Path to the standard error ouput file

module load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2

mpiexec.hydra -bootstrap slurm -n 120 ~/MyPrograms/wrf_install_intel/WRF-4.2.1/main/real.exe
```

- ndown.cs也是類似，再原來的腳本之前宣告[slurm][slurm]變數，並下載需要的模組。
- `$gfs`、`$wps`為全域變數

```bash
#!/bin/bash
#SBATCH ...

dates=();for id in {0..11};do dates=( ${dates[@]} $(date -d "$BEGD +${id}days" +%Y-%m-%d) );done
DOM=( 'CWBWRF_45k' 'SECN_9k' 'TWEPA_3k' 'tw_CWBWRF_45k' 'nests3')


i=2

cd $gfs/${DOM[$i]}/ndown

cp namelist.input23_loop namelist.input
  for cmd in "s/SYEA/$yea1/g" "s/SMON/$mon1/g" "s/SDAY/$day1/g" \
             "s/EYEA/$yea2/g" "s/EMON/$mon2/g" "s/EDAY/$day2/g" ;do
    sed -i $cmd namelist.input
  done

for hd in metoa_em wrf rsl;do if compgen -G "${hd}*" > /dev/null; then rm -f ${hd}*;fi;done

for d in 2 3;do
  dd=$(( $d - 1 ))
  for id in {0..11};do
     for j in $(ls $wps/met_em.d0${d}.${dates[$id]}_*);do
       k=${j/d0${d}/d0${dd}}
       l=${k/$wps\//}
       m=${l/met_/metoa_};ln -s $j $m;done;done;done

mpiexec.hydra -bootstrap slurm -n 120 ./real.exe
#SBATCH -o rsln.out.%j            # Path to the standard output file
#SBATCH -e rsln.error.%j          # Path to the standard error ouput file

mv wrfinput_d02 wrfndi_d02

for id in {0..11};do ln -sf $gfs/${DOM[3]}/wrfout_d02_${dates[$id]}_00:00:00 wrfout_d01_${dates[$id]}_00:00:00;done

sed -i 's/interval_seconds                    = 10800/interval_seconds                    = 3600/g' namelist.input

#ndown.exe is intel version
mpiexec.hydra -bootstrap slurm -n 120 ./ndown.exe

## restore the real and ndown results
cd $gfs/${DOM[$i]}
for f in wrfinput wrfbdy wrffdda wrflowinp;do
  mv ndown/${f}_d02 ${f}_d01
done
```

### wrf之執行

- 執行wrf之後隨即執行[trans_wrfout.py][trans]，並執行mcip

```bash
#$ cat $gfs/fcst.cs
...
    mpiexec.hydra -bootstrap slurm -n 120 ../run/wrf.exe
    $BIN/sub $BIN/trans_wrfout.py $BEGD
```

### 風場相關檔案之清理

- 執行完mcip之後，只需留存uv10及hpbl，其餘皆可刪除。

[^1]: Pratiman, 01 September 2020,Installing WRF from scratch in an HPC using Intel Compilers, see [pratiman-91.github.io][pratiman]

[pratiman]: https://pratiman-91.github.io/2020/09/01/Installing-WRF-from-scratch-in-an-HPC-using-Intel-Compilers.html "Installing WRF from scratch in an HPC using Intel Compilers"

[nchc]: https://iservice.nchc.org.tw/nchc_service/nchc_service_twn3_hpc.php "國研院國網中心台灣杉三號(Taiwania 3)為國內提供開放服務申請的最大CPU高速計算主機(2021年)，擁有900個計算節點。"
[slurm]: ../../GridModels/TWNEPA_RecommCMAQ/module_slurm.md#slurm-commands "slurm-commands"
[trans]: ../../wind_models/WRFOUT/2.TransWrfout.md "因應intel MPI轉換wrfout格式"
