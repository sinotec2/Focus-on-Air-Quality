---
layout: default
title: 國網上執行MCIP5.1
parent: Works on NCHC
grand_parent: Forecast Systems
nav_order: 3
date: 2023-03-17
last_modified_date: 2023-03-21 20:50:14
tags: forecast CMAQ nchc_service m3nc2gif
---

# 國網上執行MCIP5.1

{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>
---

## 背景

- MCIP的ㄒ

## 平行運作方案

- 特殊環境變數
  - 這些變數在所有公開範例中都沒有出現過
  - `$NPCOL_NPROW`是CCTM調控分配多核心程緒的依據
  - `$GRID_NAME`似與namelist中的`grdnam = $GridName`重複了
  - `$GRIDDESC`指得是一個輸入檔，而不是輸出檔(namelist中的`file_gd = "$FILE_GD"`)。

```bash
60,62d58
<   setenv NPCOL_NPROW '1 1'
<   setenv GRID_NAME $GridName
<   setenv GRIDDESC $DataPath/wrfout/GRIDDESC_45
250c246
< setenv IOAPI_CHECK_HEADERS  T
---
> setenv IOAPI_CHECK_HEADERS  F
```

- [slurm][slurm]的啟動
  - 因為module是bash環境下的函數，在csh環境下則為一別名（alias），在腳本中不能作用，必須回歸到原始的定義來運作。
  - `$LMOD_CMD`為內定的lmod程式
  - 詳參[csh腳本中執行module][evail]

```bash
set ProgDir    = $CMAQ_PROJ/PREP/mcip/src_5.0pnetcdf
...
eval `$LMOD_CMD tcsh load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2`  \
&& eval `$LMOD_SETTARG_CMD -s csh`
eval `$LMOD_CMD tcsh list`  && eval `$LMOD_SETTARG_CMD -s csh`
set MPI = /opt/ohpc/Taiwania3/pkg/intel/2021/mpi/2021.1.1/bin/mpiexec.hydra
mpirun -bootstrap slurm -n 1 $ProgDir/${PROG}.exe #  >& /dev/null
```

- 這個intel MPI方案的mcip fortran程式似有錯誤，無法平行運作。
  - 發生錯誤訊息：`Attempting to use an MPI routine before initializing MPICH`
  - 參考[csdn](https://blog.csdn.net/qq_37837061/article/details/123786328)網友的說法，似乎程式中沒有在單近程階段先調用mpi_init，須進入原始碼中進行偵錯。

- [slurm][slurm]啟動方式
  - [slurm][slurm]環境變數在csh中無法作動，須以sh形式啟動，另外再開啟csh環境
  - `sbatch $fcst/csh_mcip.sh 45`

```bash
$ cat csh_mcip.sh
#!/bin/sh
#SBATCH -A ENT111040             # Account name/project number
#SBATCH -J mcip                  # Job name
#SBATCH -p ct56                  # Partiotion name
#SBATCH -n 1                     # Number of MPI tasks (i.e. processes)
#SBATCH -c 1                     # Number of cores per MPI task
#SBATCH -N 1                     # Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=1      # Maximum number of tasks on each node
#SBATCH -o rsl.out.%j            # Path to the standard output file
#SBATCH -e rsl.error.%j          # Path to the standard error ouput file

module purge
csh /work/sinotec2/cmaqruns/forecast/run_mcip_DM.csh grid$1 11
```

### 序列運作方案

- 重新編譯hdf5及netcdf4，以ifort而不是mpiifort來編譯
- 將所有程式庫放在`${CMAQ_PROJ}/lib/x86_64/intel/all_noMPI`目錄下

```bash
set ProgDir    = $CMAQ_PROJ/PREP/mcip/src
...
setenv LD_LIBRARY_PATH ${CMAQ_PROJ}/lib/x86_64/intel/all_noMPI
< $ProgDir/${PROG}.exe
```

- 這支程式可以順利執行，但不接受pnetcdf寫出的檔案。需進行`$BIN/trans_wrfout.py`轉檔(詳見[因應intel MPI轉換wrfout格式][trans])。
  - 錯誤訊息：`"NCF:  NetCDF: Attempt to use feature that was not turned on when netCDF was built."`
  - build設定(libnetcdf.settings)的差異如下

項目|intel MPI|serial|說明
:-:|:-:|:-:|:-:
執行程式|wrf.exe|mcip.exe|前者為後者的上游
C Compiler|mpicc|icc|
額外程式庫|-lpnetcdf -lhdf5_hl -lhdf5 -lm -lz -lcurl|-lm -lz -lcurl|
支援NC-4平行化|是|否|
支援PnetCDF|是|否|前者具有平行化IO功能
支援DAP4[^1]|是|否|前者可以透過網路調用
支援ERANGE Fill[^2]|是|否|超出範圍時的因應方式

### 腳本中用到的.py程式

- 包括[add_firstHr.py](../MCIP/add_firstHr.md#all_in_one-version)及[brk_day.py](../../utilities/netCDF/brk_day.md)
- 使用`/opt/ohpc/pkg/rcec/pkg/python/wrfpost/bin/python`即可
- 程式中使用到ncks、ncrcat等[NCO][nco]程式，注意修改其出處路徑。

## CCTM

- 公版模式的架構把CCTM分成3段來控制：排放及網格([run.cctm.csh][1])、時間個案([project.config][2])、以及科學設定([cctm.source.v5.3.1.ae7][3])等。

### [run.cctm.csh][1]

- 空間網格在此設定，也在此分配執行節點。

```bash
setenv NPCOL_NPROW "20 10"; set NPROCS   = 200
```

### [project.config][2]

- 時間與個案在此設定。此部分不更改。
- 工作目錄的頂端也在此設定。需進行相應修正。

### [cctm.source.v5.3.1.ae7][3]

- 參考前述[MCIP 平行運作方案](#平行運作方案)及[csh腳本中執行module][evail]

```bash
  eval `$LMOD_CMD tcsh load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2`  \
  && eval `$LMOD_SETTARG_CMD -s csh`
  set MPI = /opt/ohpc/Taiwania3/pkg/intel/2021/mpi/2021.1.1/bin
  set MPIRUN = $MPI/mpiexec.hydra
  ( /usr/bin/time -p $MPIRUN -bootstrap slurm -n 200 $BLD/$EXEC ) |& tee buff_${EXECUTION_ID}.txt
```

### csh_cctm.sh

- 啟動方式：`sbatch $fcst/csh_mcip.sh 45`
- 使用ct224其中的200個單元

```bash
$ cat csh_cctm.sh
#!/bin/sh
#SBATCH -A ENT111040             # Account name/project number
#SBATCH -J cctm                  # Job name
#SBATCH -p ct224                 # Partiotion name
#SBATCH -n 200                   # Number of MPI tasks (i.e. processes)
#SBATCH -c 1                     # Number of cores per MPI task
#SBATCH -N 5                     # Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=40     # Maximum number of tasks on each node
#SBATCH -o rsl.out.%j            # Path to the standard output file
#SBATCH -e rsl.error.%j          # Path to the standard error ouput file

module purge
csh run.cctm.${1}.csh
```

### 濃度場的nest down

- 有別於[前述](#icbc)第1層東亞domain的ICBC，第2~3層的ICBC需讀取上1層的模擬結果來產生。
  - 需使用國網編譯的前處理程式BCON.BCON.exe([run_bcon_NC.csh][1dbcon])及ICON.exe([run_icon_NC.csh][icon])來接續下層的模擬。
  - 這2支程式都是intel MPI程式，需使用[slurm][slurm]來啟動。
- 腳本的引數為上一層domain**三維**的模擬結果(`CCTM_ACONC`檔)  
  - 邊界檔需逐日執行，再以ncrcat整併、最後再加上一小時。
  - 初始檔只需執行一個time frame。
- 修改項目
  - 作業位置
  - 執行檔及程式庫之位置、[啟動模組][evail]
- csh_bcon.sh、csh_icon.sh
  - bash設定[slurm][slurm] 環境變數
  - 使用ct56其中的1個process
  - 呼叫不同的csh腳本
  - 參考前述[mcip平行運作方案](#平行運作方案)

## 後處理

### 檔案清理及空間維護

- 做完濃度場的nest down後，3維濃度場(CCTM_ACONC*)及粒徑分布(CCTM_APMDIAG*)檔案可以只留存第一層，以備作圖。
- 其餘檔案如不進一步偵錯或其他用途，可以全數刪除

```bash
rm *DEP* CCTM_C* *cfg
for nc in $(ls *nc);do ncks -d LAY,0 $nc tmpnc;mv tmpnc $nc;done
```

- 檢視使用者總磁碟機用量([TWCC - III 使用手冊:儲存資源與目錄位置](https://man.twcc.ai/@TWCC-III-manual/HyOgKIiuu))
  - `/usr/lpp/mmfs/bin/mmlsquota -u sinotec2 --block-size auto fs01 fs02`
  - fs01：/work
  - fs02：/home

```bash
                         Block Limits                                    |     File Limits
Filesystem type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks
fs01       USR          110.8G       400G       500G     1.008G     none |    19790       0        0       85     none NCHC_AIcls.twcc.ai
fs02       USR          9.872G       200G       300G     488.4M     none |   194962       0        0       40     none NCHC_AIcls.twcc.ai
```

### 地面濃度圖製作

- wrf-python及cartopy之安裝
  1. 安裝conda模組 `module load pkg/Anaconda3`
  2. 開新的環境或啟用既有的環境：`conda activate ncl_stable`
  3. 安裝wrf-python：`conda install -c conda-forge wrf-python`
  4. cartopy之安裝：`pip install cartopy`
- [m3nc2gif.py](../../utilities/Graphics/wrf-python/4.m3nc2gif.md)之修改
  - 國網pnetcdf的儲存會發生錯誤，造成濃度檔有缺值(1.E30)，程式需做因應處理。
- $gfs/[make_gifs.cs][make_gifs]之修改
  - [如前](#cams數據之下載)所述，應用自行安裝套件的.py程式不能以腳本直接作為執行檔，需外加python

```bash
  for s in PM2.5 PM10 SO2 CO O3 NO2;do
    $bin/sub $pyt $bin/m3nc2gif.py $s.nc;done
```

### 結果檔案的傳輸

- 不建議傳輸大型檔案，如果需要，也必須壓縮後傳送。
- 國網上可以使用git
- 傳輸方案選擇詳見[公版模式輸入輸出檔案及傳輸管理][fs]

[^1]: > 請解釋netcdf的DAP4設定用意為何? 
NetCDF是一種廣泛使用的數據格式，用於存儲科學和工程數據。DAP4（Data Access Protocol）是NetCDF的一個擴展，它提供了更強大和靈活的數據訪問功能，使得用戶可以更加方便地使用和管理NetCDF數據。
DAP4設置的用意是為了讓NetCDF數據能夠通過DAP4協議訪問。通過DAP4，用戶可以通過網絡遠程訪問NetCDF數據，並且可以按需訪問數據的不同部分。這使得NetCDF數據更加容易共享和管理，也使得科學家和工程師更加容易訪問和使用這些數據。
具體來說，DAP4設置可以在NetCDF數據文件中定義和設置一些元數據，例如變量名稱、變量單位、維度等等，這些元數據可以通過DAP4協議訪問。此外，DAP4還提供了一些API，例如使用RESTful Web服務器將數據發佈到網絡上，以及使用ODATA等協議來管理NetCDF數據。這些API可以讓用戶更加靈活地訪問和管理NetCDF數據，並且可以將數據共享到更多的地方。
總之，DAP4設置的用意是為了使NetCDF數據更加方便地進行網絡遠程訪問，並且提供了一些API和協議，使得用戶可以更加靈活地管理和共享這些數據。這些功能可以使得NetCDF數據更加廣泛地應用於科學和工程領域，並且可以讓科學家和工程師更加容易地訪問和使用這些數據。

[^2]: > 請解釋netcdf之ERANGE Fill support用意何在?
NetCDF是一種用於存儲科學和工程數據的格式，ERANGE Fill支持是NetCDF的一種特性，它用於在寫入數據時擴展數據集的範圍，並且填充新的部分以保持數據集的一致性和完整性。
ERANGE是一個錯誤碼，它表示數據集的範圍已經達到了數據集所能容納的最大範圍，此時就無法再寫入更多的數據。在遇到這種情況時，ERANGE Fill支持可以擴展數據集的範圍，並且自動填充新的部分以容納更多的數據。這樣就可以繼續寫入數據，同時保持數據集的一致性和完整性。
ERANGE Fill支持的主要作用是在處理大型數據集時，提供一種方便的方法來管理數據集的範圍和內容，避免數據丟失或者數據集被截斷的情況發生。通過使用ERANGE Fill支持，用戶可以更加靈活地操作數據集，將數據集的範圍擴展到所需的大小，而不需要手動創建新的數據集。
總之，ERANGE Fill支持的用意是為了在NetCDF數據集寫入時提供更加靈活和自動化的管理方式，以保護數據集的完整性和一致性，同時避免數據丟失或被截斷的情況發生。

[^3]: The Climate Data Store (CDS) Application Program Interface (API) is a service providing programmatic access to CDS data. see copernicus.eu [How to use the CDS API](https://cds.climate.copernicus.eu/api-how-to)

[nchc]: https://iservice.nchc.org.tw/nchc_service/nchc_service_twn3_hpc.php "國研院國網中心台灣杉三號(Taiwania 3)為國內提供開放服務申請的最大CPU高速計算主機(2021年)，擁有900個計算節點。"
[1]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/TWNEPA_RecommCMAQ/exec/#1-主程式runcctm03csh "CMAQ Model System -> Recommend System -> 執行檔與程式庫 -> CCTM run scripts -> 1. 主程式(run.cctm.03.csh)"
[2]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/TWNEPA_RecommCMAQ/exec/#2-模擬案例與時間projectconfig "CMAQ Model System -> Recommend System -> 執行檔與程式庫 -> CCTM run scripts -> 2-模擬案例與時間project.config"
[3]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/TWNEPA_RecommCMAQ/exec/#3-科學設定檔cctmsourcev531ae7 "CMAQ Model System -> Recommend System -> 執行檔與程式庫 -> CCTM run scripts -> 3-科學設定檔cctm.source.v531.ae7"
[1dbcon]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/BCON/1day_bc/ "逐日循序執行bcon.exe"
[icon]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/ForecastSystem/10.fcst.cs/#下層icon "CMAQ Model System -> Recommend System -> 執行預報腳本之分段說明 -> CMAQ -> 下層ICON"
[make_gifs]: 15.make_gifs.md "地面濃度動畫批次製作"
[slurm]: ../../GridModels/TWNEPA_RecommCMAQ/module_slurm.md#slurm-commands "slurm-commands"
[evail]: ../TWNEPA_RecommCMAQ/module_slurm.md#csh-中執行module "csh腳本中執行module"
[trans]: ../../wind_models/WRFOUT/2.TransWrfout.md "因應intel MPI轉換wrfout格式"
[nco]: https://github.com/nco/nco "NCO NetCDF Operators@github"
[ncl]: https://www.ncl.ucar.edu/ "NCAR Command Language"
[fs]: ../../GridModels/TWNEPA_RecommCMAQ/IO_Files.md#公版模式輸入輸出檔案及傳輸管理 "公版模式輸入輸出檔案及傳輸管理"