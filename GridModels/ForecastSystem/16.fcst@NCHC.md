---
layout: default
title: 將預報系統移轉到國網
parent: Forecast System
grand_parent: CMAQ Model System
nav_order: 16
date: 2023-03-17
last_modified_date: 2023-03-21 20:50:14
tags: forecast CMAQ nchc_service m3nc2gif
---

# 將預報系統移轉到國網

{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
  {: .text-delta }
- TOC
{:toc}
</details>
---

## 背景

- 將系統移轉到[國網][nchc]似乎是個不能避免的趨勢與抉擇，[國網][nchc]與本地超微工作站的比較考量如下：

項目|[國網][nchc]|超微|說明
-|-|-|-
供電及網路穩定性|高|低|後者受到大樓內外主客觀因素干擾
軟硬體維護|專人負責|自行負責|前者含在費用之中
費用負擔|按使用收費|批次採購|後者折舊分攤沈重
儲存裝置|不提供自行備份|自行備份|前者增加傳輸困難
運維人力需求|低|高|前者只需自行負責專案部分

- 統移轉到[國網][nchc]遭遇到的困難與解決方案考量
  - pnetcdf格式相容性問題：
    - 詳[轉換wrfout格式](https://sinotec2.github.io/FAQ/2023/03/17/TransWrfout.html)
  - 磁碟機使用上限
    - wrf執行需要近80G、cmaq執行一個domain也會需要近200G。
    - 目前以手動方式一面執行、一面清理方式進行。如果要自動連續執行，需有外部磁碟機連線(如aws s3)才行。
  - crontab的替代
    - 國網並未開放crontab的使用
    - 目前似乎還沒有好的替代方案，或許只能手動啟動、tmux + while true方案。

## ICBC

- 因ICBC的準備主要是下載、檔案轉換，並未涉及大量的計算，並未使用slurm。

### CAMS數據之下載

- 檔案下載會需要使用到cdsapi模組、需另行下載。
  - 詳[歐洲中期天氣預報中心再分析數據之下載](../../AQana/GAQuality/ECMWF_rean/EC_ReAna.md)
- 國網的作法
  - 裝置Anaconda環境：`module load pkg/Anaconda3`
  - 開始一個空的conda環境：`conda env create gribby`
  - 裝置cdsapi：`pip install cdsapi`
  - 國網python套件是裝置在個人的家目錄下(`.conda`)，因此執行上須以對應之python(`$py`)來啟動程式，無法以執行檔直接執行方式來執行。

```bash
pyt=/home/sinotec2/.conda/envs/gribby/bin/python
...
   $pyt get_All.py y $dt $hr $i >& /dev/null
...
```

- 因CAMS採會員制，會員帳密訊息必須儲存在個人的家目錄(`.cdsapirc`)，因此無法由不同使用者執行同一目錄工作。

### NCO 程式

- 國網並沒有提供NCO程式。此處將其放一份在`$BIN=/work/sinotec2/opt/cmaq_recommend/bin/`下。
- 一般的NCO程式並不包括`ncl_convert2nc`，此為ncl_stable套件內容

### 初始檔案

- 只修正目錄系統
- 因未有特殊的模組，可以用公用wrfpost環境下的python

```bash
sinotec2@lgn303 /work/sinotec2/CAMS
$ diff grb2icon.py ~/bin/grb2icon.py
1c1
< #!/opt/ohpc/pkg/rcec/pkg/python/wrfpost/bin/python
---
> #!/opt/anaconda3/envs/py37/bin/python
72c72
< fname='/work/sinotec2/cmaqruns/forecast/grid45/mcip/DENS/METCRO3D.'+bdate.strftime('%Y%m%d')
---
> fname='/u01/cmaqruns/2022fcst/grid45/mcip/DENS/METCRO3D.'+bdate.strftime('%Y%m%d')
79c79
< targ='/work/sinotec2/cmaqruns/forecast/grid45/icon/'
---
> targ='/u01/cmaqruns/2022fcst/grid45/icon/'
```

### 邊界條件

- 修正相應之目錄系統
- 密度因不會太敏感，原來沒有讀取確切的逐日模擬結果。此處恢復讀取。

```bash
$ diff grb2bcon.py ~/bin/grb2bcon.py
1c1
< #!/opt/ohpc/pkg/rcec/pkg/python/wrfpost/bin/python
---
> #!/opt/anaconda3/envs/py37/bin/python
92d91
< targ='/work/sinotec2/cmaqruns/forecast/grid45/bcon/'
94,95c93
< adate=sys.argv[1].replace('-','')
< fname=targ.replace('bcon','mcip')+'/DENS/METCRO3D.'+adate
---
> fname='METCRO3D.nc'
116a115
> targ='/u01/cmaqruns/2022fcst/grid45/bcon/'
```

## 風場之準備

- 這部分有很大的差異，除了real(ndown)及wrf需要平行計算之外，WPS及後處理沒有再使用slurm。
- wrfout檔案格式轉換及mcip包裝在一起，放在背景執行，不影響程序的進行。

### gfs數據下載與處理

- 國網下載很快，不需要平行運作，依序進行即可，另建一個批次檔get_gfs.cs。

```bash
#sinotec2@lgn303 /work/sinotec2/WRF4/WRF4.2.1
#$ cat  $gfs/get_gfs.cs
wget=/usr/bin/wget
root=https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.
BH=00
dir=$begdp/$BH/atmos/

cd $gfs

for ((i=0;i <= 312; i+=3));do
  iii=$(printf "%03d" $i)
  file=gfs.t${BH}z.pgrb2.1p00.f$iii
  if [ -e $file ];then rm $file;fi
  $wget --no-check-certificate -q --retry-connrefused --waitretry=3 --random-wait \
        --read-timeout=20 --timeout=15 -t 10 --continue $root$dir$file
done
```

- 序列處理，因此置換日期是全段模擬的起迄日，不必再每個timeframe分開做。

```bash
./link_grib.csh gfs*
hh=00
cp namelist.wps_loop namelist.wps
for cmd in 's/BEGD/'$BEGD'/g' 's/ENDD/'$ENDD'/g' 's/HH/'$hh'/g';do sed -ie $cmd namelist.wps;done
../UGB
```

- wps系列程式的編譯是用ifort，外加自行編譯的jasper與png，因此UGB的內容也有所不同。

```bash
module purge
module load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/work/sinotec2/opt/jasper/lib:/work/sinotec2/opt/png/lib ./ungrib.exe
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/work/sinotec2/opt/jasper/lib:/work/sinotec2/opt/png/lib ./metgrid.exe
```

### real及ndown

- 使用3個node，每個node上執行40個程式，因此總程序為120 

```bash
sinotec2@lgn303 /work/sinotec2/WRF4
$ cat ./WRF4.2.1/doreal
#!/bin/bash
#SBATCH -A ENT111040             # Account name/project number
#SBATCH -J wrf                   # Job name
#SBATCH -p ct224                 # Partiotion name
#SBATCH -n 120                   # Number of MPI tasks (i.e. processes)
#SBATCH -c 1                     # Number of cores per MPI task
#SBATCH -N 3                     # Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=40     # Maximum number of tasks on each node
#SBATCH -o rsl.out.%j            # Path to the standard output file
#SBATCH -e rsl.error.%j          # Path to the standard error ouput file

module load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2

mpiexec.hydra -bootstrap slurm -n 120 ~/MyPrograms/wrf_install_intel/WRF-4.2.1/main/real.exe
```

- ndown.cs也是類似，再原來的腳本之前宣告slurm變數，並下載需要的模組。
- `$gfs`、`$wps`為全域變數

```bash
#!/bin/bash
#SBATCH -A ENT111040             # Account name/project number
#SBATCH -J ndown                 # Job name
#SBATCH -p ct224                 # Partiotion name
#SBATCH -n 120                   # Number of MPI tasks (i.e. processes)
#SBATCH -c 1                     # Number of cores per MPI task
#SBATCH -N 3                     # Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=40     # Maximum number of tasks on each node
#SBATCH -o rsl.out.%j            # Path to the standard output file
#SBATCH -e rsl.error.%j          # Path to the standard error ouput file

module load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2


dates=();for id in {0..11};do dates=( ${dates[@]} $(date -d "$BEGD +${id}days" +%Y-%m-%d) );done
DOM=( 'CWBWRF_45k' 'SECN_9k' 'TWEPA_3k' 'tw_CWBWRF_45k' 'nests3')


i=2

cd $gfs/${DOM[$i]}/ndown

cp namelist.input23_loop namelist.input
  for cmd in "s/SYEA/$yea1/g" "s/SMON/$mon1/g" "s/SDAY/$day1/g" \
             "s/EYEA/$yea2/g" "s/EMON/$mon2/g" "s/EDAY/$day2/g" ;do
    sed -i $cmd namelist.input
  done

for hd in metoa_em wrf rsl;do if compgen -G "${hd}*" > /dev/null; then rm -f ${hd}*;fi;done

for d in 2 3;do
  dd=$(( $d - 1 ))
  for id in {0..11};do
     for j in $(ls $wps/met_em.d0${d}.${dates[$id]}_*);do
       k=${j/d0${d}/d0${dd}}
       l=${k/$wps\//}
       m=${l/met_/metoa_};ln -s $j $m;done;done;done

mpiexec.hydra -bootstrap slurm -n 120 ./real.exe
#SBATCH -o rsln.out.%j            # Path to the standard output file
#SBATCH -e rsln.error.%j          # Path to the standard error ouput file

mv wrfinput_d02 wrfndi_d02

for id in {0..11};do ln -sf $gfs/${DOM[3]}/wrfout_d02_${dates[$id]}_00:00:00 wrfout_d01_${dates[$id]}_00:00:00;done

sed -i 's/interval_seconds                    = 10800/interval_seconds                    = 3600/g' namelist.input

#ndown.exe is intel version
mpiexec.hydra -bootstrap slurm -n 120 ./ndown.exe

## restore the real and ndown results
cd $gfs/${DOM[$i]}
for f in wrfinput wrfbdy wrffdda wrflowinp;do
  mv ndown/${f}_d02 ${f}_d01
done
```

### wrf之執行

- 執行wrf之後隨即執行trans_wrfout.py，並執行mcip

```bash
#$ cat $gfs/fcst.cs
...
    mpiexec.hydra -bootstrap slurm -n 120 ../run/wrf.exe
    $BIN/sub $BIN/trans_wrfout.py $BEGD
```

### 風場相關檔案之清理

- 執行完mcip之後，只需留存uv10及hpbl，其餘皆可刪除。

## MCIP

### 平行運作方案

- 特殊環境變數

```bash
60,62d58
<   setenv NPCOL_NPROW '1 1'
<   setenv GRID_NAME $GridName
<   setenv GRIDDESC $DataPath/wrfout/GRIDDESC_45
250c246
< setenv IOAPI_CHECK_HEADERS  T
---
> setenv IOAPI_CHECK_HEADERS  F
```

- slurm的啟動
  - 因為module是bash環境下的函數，在csh環境下則為一別名（alias），在腳本中不能作用，必須回歸到原始的定義來運作。
  - `$LMOD_CMD`為內定的環境變數

```bash
set ProgDir    = $CMAQ_PROJ/PREP/mcip/src_5.0pnetcdf
...
eval `$LMOD_CMD tcsh load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2`  \
&& eval `$LMOD_SETTARG_CMD -s csh`
eval `$LMOD_CMD tcsh list`  && eval `$LMOD_SETTARG_CMD -s csh`
set MPI = /opt/ohpc/Taiwania3/pkg/intel/2021/mpi/2021.1.1/bin/mpiexec.hydra
mpirun -bootstrap slurm -n 1 $ProgDir/${PROG}.exe #  >& /dev/null
```

- 這個intel mpi方案的mcip fortran程式似有錯誤，無法平行運作。
- slurm啟動方式
  - slurm環境變數在csh中無法作動，須以sh形式啟動，另外再開啟csh環境
  - `sbatch $fcst/csh_mcip.sh 45`

```bash
$ cat csh_mcip.sh
#!/bin/sh
#SBATCH -A ENT111040             # Account name/project number
#SBATCH -J mcip                  # Job name
#SBATCH -p ct56                  # Partiotion name
#SBATCH -n 1                     # Number of MPI tasks (i.e. processes)
#SBATCH -c 1                     # Number of cores per MPI task
#SBATCH -N 1                     # Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=1      # Maximum number of tasks on each node
#SBATCH -o rsl.out.%j            # Path to the standard output file
#SBATCH -e rsl.error.%j          # Path to the standard error ouput file

module purge
csh /work/sinotec2/cmaqruns/forecast/run_mcip_DM.csh grid$1 11
```

### 序列運作方案

- 重新編譯hdf5及netcdf4，以ifort而不是mpiifort來編譯
- 將所有程式庫放在`${CMAQ_PROJ}/lib/x86_64/intel/all_noMPI`目錄下

```bash
set ProgDir    = $CMAQ_PROJ/PREP/mcip/src
...
setenv LD_LIBRARY_PATH ${CMAQ_PROJ}/lib/x86_64/intel/all_noMPI
< $ProgDir/${PROG}.exe
```

- 這支程式可以順利執行，但不接受pnetcdf寫出的檔案。需進行`$BIN/trans_wrfout.py`轉檔。

### 腳本中用到的.py程式

- 包括[add_firstHr.py](../MCIP/add_firstHr.md#all_in_one-version)及[brk_day.py](../../utilities/netCDF/brk_day.md)
- 使用`/opt/ohpc/pkg/rcec/pkg/python/wrfpost/bin/python`即可
- 程式中使用到ncks、ncrcat等NCO程式，注意修改其出處路徑。

## CCTM

- 公版模式的架構把CCTM分成3段來控制：排放及網格([run.cctm.csh][1])、時間個案([project.config][2])、以及科學設定([cctm.source.v5.3.1.ae7][3])等。

### [run.cctm.csh][1]

- 空間網格在此設定，也在此分配執行節點。

```bash
setenv NPCOL_NPROW "20 10"; set NPROCS   = 200
```

### [project.config][2]

- 時間與個案在此設定。此部分不更改。
- 工作目錄的頂端也在此設定。需進行相應修正。

### [cctm.source.v5.3.1.ae7][3]

- 參考前述[MCIP 平行運作方案](#平行運作方案)

```bash
  eval `$LMOD_CMD tcsh load compiler/intel/2021 IntelMPI/2021 hdf5/1.12 netcdf/4.7.4 pnetcdf/1.12.2`  \
  && eval `$LMOD_SETTARG_CMD -s csh`
  set MPI = /opt/ohpc/Taiwania3/pkg/intel/2021/mpi/2021.1.1/bin
  set MPIRUN = $MPI/mpiexec.hydra
  ( /usr/bin/time -p $MPIRUN -bootstrap slurm -n 200 $BLD/$EXEC ) |& tee buff_${EXECUTION_ID}.txt
```

### csh_cctm.sh

- 啟動方式：`sbatch $fcst/csh_mcip.sh 45`
- 使用ct224其中的200個單元

```bash
$ cat csh_cctm.sh
#!/bin/sh
#SBATCH -A ENT111040             # Account name/project number
#SBATCH -J cctm                  # Job name
#SBATCH -p ct224                 # Partiotion name
#SBATCH -n 200                   # Number of MPI tasks (i.e. processes)
#SBATCH -c 1                     # Number of cores per MPI task
#SBATCH -N 5                     # Maximum number of nodes to be allocated
#SBATCH --ntasks-per-node=40     # Maximum number of tasks on each node
#SBATCH -o rsl.out.%j            # Path to the standard output file
#SBATCH -e rsl.error.%j          # Path to the standard error ouput file

module purge
csh run.cctm.${1}.csh
```

### 濃度場的nest down

- 上層CCTM執行完後，須執行BCON.exe([run_bcon_NC.csh][1dbcon])及ICON.exe([run_icon_NC.csh][icon])，來接續下層的模擬。
- 引述為上層模擬結果(`CCTM_ACONC`檔)
- 修改項目
  - 作業位置
  - 執行檔及程式庫之位置、啟動模組
- csh_bcon.sh、csh_icon.sh
  - bash設定slurm 環境變數
  - 使用ct56其中的1個process
  - 呼叫不同的腳本

## 後處理

### 檔案清理及空間維護

- 做完濃度場的nest down後，3維濃度場及粒徑分布檔案可以只留存第一層，以備作圖。
- 其餘檔案如不進一步偵錯或其他用途，可以全數刪除

```bash
rm *DEP* CCTM_C* *cfg
for nc in $(ls *nc);do ncks -d LAY,0 $nc tmpnc;mv tmpnc $nc;done
```

- 檢視使用者總磁碟機用量([TWCC - III 使用手冊:儲存資源與目錄位置](https://man.twcc.ai/@TWCC-III-manual/HyOgKIiuu))
  - `/usr/lpp/mmfs/bin/mmlsquota -u sinotec2 --block-size auto fs01 fs02`
  - fs01：/work
  - fs02：/home

```bash
                         Block Limits                                    |     File Limits
Filesystem type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks
fs01       USR          110.8G       400G       500G     1.008G     none |    19790       0        0       85     none NCHC_AIcls.twcc.ai
fs02       USR          9.872G       200G       300G     488.4M     none |   194962       0        0       40     none NCHC_AIcls.twcc.ai
```

### 地面濃度圖製作

- wrf-python及cartopy之安裝
  1. 安裝conda模組 `module load pkg/Anaconda3`
  2. 開新的環境或啟用既有的環境：`conda activate ncl_stable`
  3. 安裝wrf-python：`conda install -c conda-forge wrf-python`
  4. cartopy之安裝：`pip install cartopy`
- [m3nc2gif.py](../../utilities/Graphics/wrf-python/4.m3nc2gif.md)之修改
  - 國網pnetcdf的儲存會發生錯誤，造成濃度檔有缺值(1.E30)，程式需做因應處理。
- $gfs/[make_gifs.cs][make_gifs]之修改
  - 如前所述，.py程式不能自己作為執行檔，需外加python

```bash
  for s in PM2.5 PM10 SO2 CO O3 NO2;do
    $bin/sub $pyt $bin/m3nc2gif.py $s.nc;done
```

[nchc]: https://iservice.nchc.org.tw/nchc_service/nchc_service_twn3_hpc.php "國研院國網中心台灣杉三號(Taiwania 3)為國內提供開放服務申請的最大CPU高速計算主機(2021年)，擁有900個計算節點。"
[1]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/TWNEPA_RecommCMAQ/exec/#1-主程式runcctm03csh "CMAQ Model System -> Recommend System -> 執行檔與程式庫 -> CCTM run scripts -> 1. 主程式(run.cctm.03.csh)"
[2]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/TWNEPA_RecommCMAQ/exec/#2-模擬案例與時間projectconfig "CMAQ Model System -> Recommend System -> 執行檔與程式庫 -> CCTM run scripts -> 2-模擬案例與時間project.config"
[3]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/TWNEPA_RecommCMAQ/exec/#3-科學設定檔cctmsourcev531ae7 "CMAQ Model System -> Recommend System -> 執行檔與程式庫 -> CCTM run scripts -> 3-科學設定檔cctm.source.v531.ae7"
[1dbcon]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/BCON/1day_bc/ "逐日循序執行bcon.exe"
[icon]: https://sinotec2.github.io/Focus-on-Air-Quality/GridModels/ForecastSystem/10.fcst.cs/#下層icon "CMAQ Model System -> Recommend System -> 執行預報腳本之分段說明 -> CMAQ -> 下層ICON"
[make_gifs]: 15.make_gifs.md "地面濃度動畫批次製作"
